{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Byte-Pair Encoding"
      ],
      "metadata": {
        "id": "f3YzXQS7hYfp"
      },
      "id": "f3YzXQS7hYfp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About"
      ],
      "metadata": {
        "id": "6nBSJ-Rphc-3"
      },
      "id": "6nBSJ-Rphc-3"
    },
    {
      "cell_type": "markdown",
      "id": "38d5b411",
      "metadata": {
        "id": "38d5b411"
      },
      "source": [
        "Byte-Pair Encoding (BPE) was initially developed as an algorithm to compress texts, and then used by OpenAI for tokenization when pretraining the GPT model. It’s used by a lot of Transformer models, including GPT, GPT-2, RoBERTa, BART, and DeBERTa."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "jgkLmB8Qhtfo"
      },
      "id": "jgkLmB8Qhtfo"
    },
    {
      "cell_type": "markdown",
      "id": "d377ac51",
      "metadata": {
        "id": "d377ac51"
      },
      "source": [
        "BPE training starts by computing the unique set of words used in the corpus (after the normalization and pre-tokenization steps are completed), then building the vocabulary by taking all the symbols used to write those words.\n",
        "\n",
        "For real-world cases, base vocabulary will contain all the ASCII characters, at the very least, and probably some Unicode characters as well. If an example you are tokenizing uses a character that is not in the training corpus, that character will be converted to the unknown token.\n",
        "\n",
        "Now we add new tokens until the desired vocabulary size is reached by learning merges, which are rules to merge two elements of the existing vocabulary together into a new one.\n",
        "\n",
        "At any step during the tokenizer training, the BPE algorithm will search for the most frequent pair of existing tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example"
      ],
      "metadata": {
        "id": "PxXe5QakiBSL"
      },
      "id": "PxXe5QakiBSL"
    },
    {
      "cell_type": "markdown",
      "id": "7815a4d7",
      "metadata": {
        "id": "7815a4d7"
      },
      "source": [
        "As a very simple example, let’s say our corpus uses these five words:\n",
        "\n",
        "`\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"`\n",
        "\n",
        "The base vocabulary will then be [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"].\n",
        "\n",
        "let’s assume the words had the following frequencies:\n",
        "\n",
        "`(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)`\n",
        "\n",
        "meaning \"hug\" was present 10 times in the corpus, \"pug\" 5 times, \"pun\" 12 times, \"bun\" 4 times, and \"hugs\" 5 times. We start the training by splitting each word into characters (the ones that form our initial vocabulary) so we can see each word as a list of tokens:\n",
        "\n",
        "`(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)`\n",
        "\n",
        "Then we look at pairs. The pair (\"h\", \"u\") is present in the words \"hug\" and \"hugs\", so 15 times total in the corpus. It’s not the most frequent pair, though: that honor belongs to (\"u\", \"g\"), which is present in \"hug\", \"pug\", and \"hugs\", for a grand total of 20 times in the vocabulary.\n",
        "\n",
        "Thus, the first merge rule learned by the tokenizer is (\"u\", \"g\") -> \"ug\", which means that \"ug\" will be added to the vocabulary, and the pair should be merged in all the words of the corpus. At the end of this stage, the vocabulary and corpus look like this:\n",
        "\n",
        "`Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"]`\n",
        "\n",
        "`Corpus: (\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)`\n",
        "\n",
        "Now we have some pairs that result in a token longer than two characters: the pair (\"h\", \"ug\"), for instance (present 15 times in the corpus). The most frequent pair at this stage is (\"u\", \"n\"), however, present 16 times in the corpus, so the second merge rule learned is (\"u\", \"n\") -> \"un\". Adding that to the vocabulary and merging all existing occurrences leads us to:\n",
        "\n",
        "`Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\"]`\n",
        "\n",
        "`Corpus: (\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"h\" \"ug\" \"s\", 5)`\n",
        "\n",
        "Now the most frequent pair is (\"h\", \"ug\"), so we learn the merge rule (\"h\", \"ug\") -> \"hug\", which gives us our first three-letter token. After the merge, the corpus looks like this:\n",
        "\n",
        "`Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]`\n",
        "\n",
        "`Corpus: (\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)`\n",
        "\n",
        "And we continue like this until we reach the desired vocabulary size."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization Algorithm"
      ],
      "metadata": {
        "id": "bPp2a8R-iOL1"
      },
      "id": "bPp2a8R-iOL1"
    },
    {
      "cell_type": "markdown",
      "id": "4df18e41",
      "metadata": {
        "id": "4df18e41"
      },
      "source": [
        "Tokenization follows the training process closely, in the sense that new inputs are tokenized by applying the following steps:\n",
        "\n",
        "1.   Normalization\n",
        "2.   Pre-tokenization\n",
        "3.   Splitting the words into individual characters\n",
        "4.   Applying the merge rules learned in order on those splits\n",
        "\n",
        "Let’s take the example we used during training, with the three merge rules learned:\n",
        "\n",
        "`(\"u\", \"g\") -> \"ug\"`\n",
        "\n",
        "`(\"u\", \"n\") -> \"un\"`\n",
        "\n",
        "`(\"h\", \"ug\") -> \"hug\"`\n",
        "\n",
        "The word \"bug\" will be tokenized as [\"b\", \"ug\"]. \"mug\", however, will be tokenized as [\"[UNK]\", \"ug\"] since the letter \"m\" was not in the base vocabulary. Likewise, the word \"thug\" will be tokenized as [\"[UNK]\", \"hug\"]: the letter \"t\" is not in the base vocabulary, and applying the merge rules results first in \"u\" and \"g\" being merged and then \"h\" and \"ug\" being merged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7e9ca55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7e9ca55",
        "outputId": "8da3bb87-d695-41ac-fb6a-9d62bc37a5bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [1212, 318, 257, 6291, 6827, 13], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "sentence = \"This is a sample sentence.\"\n",
        "tokens = tokenizer(sentence)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mshojaei77/PersianBPETokenizer\")\n",
        "test_sentence = \"سلام، چطور هستید؟ امیدوارم روز خوبی داشته باشید\"\n",
        "tokens = tokenizer.tokenize(test_sentence)\n",
        "print(\"Tokens:\", tokens)\n",
        "encoded = tokenizer(test_sentence)\n",
        "print(\"Input IDs:\", encoded[\"input_ids\"])\n",
        "print(\"Decoded:\", tokenizer.decode(encoded[\"input_ids\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmTOmvWhn6uw",
        "outputId": "a6a8f208-6c4f-46fd-ba6e-20a6c1c69820"
      },
      "id": "GmTOmvWhn6uw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['سلام', '،', 'چطور', 'هستید', '؟', 'امیدوارم', 'روز', 'خوبی', 'داشته', 'باشید']\n",
            "Input IDs: [1, 3007, 241, 3167, 3883, 243, 4526, 2063, 2750, 2297, 2863, 2]\n",
            "Decoded: \" سلام ، چطور هستید ؟ امیدوارم روز خوبی داشته باشید #\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# corpus = [\n",
        "#     \"This is the Hugging Face Course.\",\n",
        "#     \"This chapter is about tokenization.\",\n",
        "#     \"This section shows several tokenizer algorithms.\",\n",
        "#     \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
        "# ]\n",
        "corpus = [\"این یک عبارت برای تست روش مورد بررسی است.\",\n",
        "          \"در این مدل سعی در توکنایز کردن این عبارت داریم.\"\n",
        "]\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "\n",
        "for text in corpus:\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    for word in new_words:\n",
        "        word_freqs[word] += 1\n",
        "\n",
        "print(word_freqs)"
      ],
      "metadata": {
        "id": "3hbEY58UhJ2M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3b1249-ce32-411a-8ab1-9ba2bfe9db92"
      },
      "id": "3hbEY58UhJ2M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {'این': 3, 'یک': 1, 'عبارت': 2, 'برای': 1, 'تست': 1, 'روش': 1, 'مورد': 1, 'بررسی': 1, 'است': 1, '.': 2, 'در': 2, 'مدل': 1, 'سعی': 1, 'توکنایز': 1, 'کردن': 1, 'داریم': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet = []\n",
        "\n",
        "for word in word_freqs.keys():\n",
        "    for letter in word:\n",
        "        if letter not in alphabet:\n",
        "            alphabet.append(letter)\n",
        "alphabet.sort()\n",
        "\n",
        "vocab = [\"<|endoftext|>\"] + alphabet.copy()\n",
        "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
        "\n",
        "\n",
        "print(alphabet)\n",
        "print(splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjKEt8pHkOsK",
        "outputId": "8523879d-26a8-4559-bd85-2f7428d1dd08"
      },
      "id": "FjKEt8pHkOsK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.', 'ا', 'ب', 'ت', 'د', 'ر', 'ز', 'س', 'ش', 'ع', 'ل', 'م', 'ن', 'و', 'ک', 'ی']\n",
            "{'این': ['ا', 'ی', 'ن'], 'یک': ['ی', 'ک'], 'عبارت': ['ع', 'ب', 'ا', 'ر', 'ت'], 'برای': ['ب', 'ر', 'ا', 'ی'], 'تست': ['ت', 'س', 'ت'], 'روش': ['ر', 'و', 'ش'], 'مورد': ['م', 'و', 'ر', 'د'], 'بررسی': ['ب', 'ر', 'ر', 'س', 'ی'], 'است': ['ا', 'س', 'ت'], '.': ['.'], 'در': ['د', 'ر'], 'مدل': ['م', 'د', 'ل'], 'سعی': ['س', 'ع', 'ی'], 'توکنایز': ['ت', 'و', 'ک', 'ن', 'ا', 'ی', 'ز'], 'کردن': ['ک', 'ر', 'د', 'ن'], 'داریم': ['د', 'ا', 'ر', 'ی', 'م']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_pair_freqs(splits):\n",
        "    pair_freqs = defaultdict(int)\n",
        "    for word, freq in word_freqs.items():\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "        for i in range(len(split) - 1):\n",
        "            pair = (split[i], split[i + 1])\n",
        "            pair_freqs[pair] += freq\n",
        "    return pair_freqs\n",
        "\n",
        "pair_freqs = compute_pair_freqs(splits)\n",
        "\n",
        "for i, key in enumerate(pair_freqs.keys()):\n",
        "    print(f\"{key}: {pair_freqs[key]}\")\n",
        "    if i >= 5:\n",
        "        break\n",
        "\n",
        "best_pair = \"\"\n",
        "max_freq = None\n",
        "\n",
        "for pair, freq in pair_freqs.items():\n",
        "    if max_freq is None or max_freq < freq:\n",
        "        best_pair = pair\n",
        "        max_freq = freq\n",
        "\n",
        "print(best_pair, max_freq)\n",
        "\n",
        "merges = {(\"ی\", \"ا\"): \"ای\"}\n",
        "vocab.append(\"ای\")\n",
        "\n",
        "def merge_pair(a, b, splits):\n",
        "    for word in word_freqs:\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "\n",
        "        i = 0\n",
        "        while i < len(split) - 1:\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "                split = split[:i] + [a + b] + split[i + 2 :]\n",
        "            else:\n",
        "                i += 1\n",
        "        splits[word] = split\n",
        "    return splits\n",
        "\n",
        "splits = merge_pair(\"ی\", \"ا\", splits)\n",
        "print(splits[\"این\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNuiOjwpk_F-",
        "outputId": "ec0e2e33-a7de-4633-8fb4-136af5415878"
      },
      "id": "xNuiOjwpk_F-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('ا', 'ی'): 5\n",
            "('ی', 'ن'): 3\n",
            "('ی', 'ک'): 1\n",
            "('ع', 'ب'): 2\n",
            "('ب', 'ا'): 2\n",
            "('ا', 'ر'): 3\n",
            "('ا', 'ی') 5\n",
            "['ا', 'ی', 'ن']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50\n",
        "\n",
        "while len(vocab) < vocab_size:\n",
        "    pair_freqs = compute_pair_freqs(splits)\n",
        "    best_pair = \"\"\n",
        "    max_freq = None\n",
        "    for pair, freq in pair_freqs.items():\n",
        "        if max_freq is None or max_freq < freq:\n",
        "            best_pair = pair\n",
        "            max_freq = freq\n",
        "    splits = merge_pair(*best_pair, splits)\n",
        "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
        "    vocab.append(best_pair[0] + best_pair[1])\n",
        "\n",
        "print(merges)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07H6cQ9qmbf2",
        "outputId": "40e42619-86f7-4ebb-a9b2-6c91e39c6be4"
      },
      "id": "07H6cQ9qmbf2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('ی', 'ا'): 'ای', ('ا', 'ی'): 'ای', ('ای', 'ن'): 'این', ('ا', 'ر'): 'ار', ('ع', 'ب'): 'عب', ('عب', 'ار'): 'عبار', ('عبار', 'ت'): 'عبارت', ('ب', 'ر'): 'بر', ('س', 'ت'): 'ست', ('ر', 'د'): 'رد', ('د', 'ر'): 'در', ('ی', 'ک'): 'یک', ('بر', 'ای'): 'برای', ('ت', 'ست'): 'تست', ('ر', 'و'): 'رو', ('رو', 'ش'): 'روش', ('م', 'و'): 'مو', ('مو', 'رد'): 'مورد', ('بر', 'ر'): 'برر', ('برر', 'س'): 'بررس', ('بررس', 'ی'): 'بررسی', ('ا', 'ست'): 'است', ('م', 'د'): 'مد', ('مد', 'ل'): 'مدل', ('س', 'ع'): 'سع', ('سع', 'ی'): 'سعی', ('ت', 'و'): 'تو', ('تو', 'ک'): 'توک', ('توک', 'ن'): 'توکن', ('توکن', 'ای'): 'توکنای', ('توکنای', 'ز'): 'توکنایز', ('ک', 'رد'): 'کرد'}\n",
            "['<|endoftext|>', '.', 'ا', 'ب', 'ت', 'د', 'ر', 'ز', 'س', 'ش', 'ع', 'ل', 'م', 'ن', 'و', 'ک', 'ی', 'ای', 'ای', 'ای', 'این', 'ار', 'عب', 'عبار', 'عبارت', 'بر', 'ست', 'رد', 'در', 'یک', 'برای', 'تست', 'رو', 'روش', 'مو', 'مورد', 'برر', 'بررس', 'بررسی', 'است', 'مد', 'مدل', 'سع', 'سعی', 'تو', 'توک', 'توکن', 'توکنای', 'توکنایز', 'کرد']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
        "    for pair, merge in merges.items():\n",
        "        for idx, split in enumerate(splits):\n",
        "            i = 0\n",
        "            while i < len(split) - 1:\n",
        "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
        "                    split = split[:i] + [merge] + split[i + 2 :]\n",
        "                else:\n",
        "                    i += 1\n",
        "            splits[idx] = split\n",
        "\n",
        "    return sum(splits, [])\n",
        "\n",
        "tokenize(\"این یک عبارت است.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP7Bbct-msAf",
        "outputId": "1ac07acc-a38d-4a22-89ce-f7e25d916046"
      },
      "id": "GP7Bbct-msAf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['این', 'یک', 'عبارت', 'است', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}